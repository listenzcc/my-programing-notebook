# ViT注意力map的计算过程

Title: ViT注意力map的计算过程
Date: 2024-4-24
Keywords: Transformer, Attention

---

本文对ViT注意力map的计算方法进行简要介绍。是否重要我不好评估，但真的十分简单。

[toc]

---

## 计算过程简述

由于注意力机制有两个维度，分别是transformer神经网络的层数（$H \in N^+$）和注意力的头数（$L \in N^+$）。为了表示方便，用中括号表示整数的集合

$$
[L] = [1, 2, \dots, L]
$$

$H$同理。每层和每个注意力头的注意力用下式计算

$$
A_{l, h}=softmax\begin{pmatrix}\frac{Q_{l, h} K_{l, h}^T}{\sqrt{D}}\end{pmatrix}, (l, h) \in [L], [H]
$$

其中，$A_{l, h}\in R^{(N+1) \times (N+1)}$即是的注意力矩阵，$N$代表图像被分解为$N=16\times 16$个小块（patches）。令每层的注意力等于它的全部注意力头的平均

$$
\hat{A}_l=\frac{1}{H}\sum_{h\in [H]} A_{l, h}, l \in [L]
$$

对于不同层的注意力，需要额外考虑残差连接，它是单位矩阵（$I$）

$$
\hat{A}=\prod_{l\in[L]}\begin{pmatrix} \hat{A}_l + I\end{pmatrix}
$$

最后，该矩阵的第一行就代表每个patch的注意力大小

$$
A = \hat{A}_{1, 2:N+1}, A \in R^{N \times 1}
$$

![Untitled](ViT%E6%B3%A8%E6%84%8F%E5%8A%9Bmap%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%2030b07f45178441099c9dbc33db7798b0/Untitled.png)

## 附录：参考材料

[Statistical Test for Attention Map in Vision Transformer](https://arxiv.org/abs/2401.08169)
