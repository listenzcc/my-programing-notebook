# Transformer大模型的位置编码

Title: Transformer大模型的位置编码
Date: 2024-4-13
Keywords: Transformer

---

本文介绍了Transformer模型中的经典位置编码方式，通过使用正弦和余弦函数对位置进行编码。作者分析了不同位置之间的关系，并探讨了参数对位置编码的影响。通过距离矩阵和相关系数的计算，展示了不同位置编码之间的差异程度。最后，绘制了几个典型位置编码与其他位置编码之间的距离图。详细内容可查看相关链接。

[toc]

---

## 元素在序列中的位置

以ChatGPT、Bert为代表的大模型在自然语言处理、图像处理等方面的处理效果出类拔萃。究其原因是它们使用了transformer结构的注意力机制。所谓注意力机制就是不再单纯地对输入信息进行向量化展开，而是需要输入元素之间的复杂关系，这里至少包含两类关系，一类是元素的位置关系，另一类是元素的内容关联。先说位置关系。

那么，什么是位置关系？这里举两个例子

- 你说，我明天不去上学？
- 你说，我明天去上学不？

其中， “去上学“和“不”就构成了两组不同的位置关系。由于它们位置关系的不同，首句就是反问句，代表对”不去上学“的事实或者动议表达诧异；次句就是一般疑问句，它的回答有”是、否“两种，代表一个半开放性的问题，等待对方的建议或指令。

另外，句中的“我”和“去上学”之间显然具有强烈的语义连接关系，而“你”则没有形成这个关联。这显然也是需要模型对“你”和“我”的位置进行良好的、可计算的编码，才能在后续计算中体现这些信息。

## Transformer的经典位置编码方式

这是一个很大的话题，本文仅对transformer模型中的经典位置编码方式进行计算和展示。在注意力机制的首文中，采用如下编码方式

$$
\begin{cases}
PE_{p, 2i} &= sin(\frac{p}{10000^{2i/d}})\\
PE_{p, 2i+1} &= cos(\frac{p}{10000^{2i/d}})
\end{cases}
$$

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

其中，脚标$p, 2i$代表位置$p=2i$是偶数，脚标$p, 2i+1$代表位置$p=2i+1$是奇数。这相当于完成了这样一个映射

$$
\begin{cases}
PE &: PositionEncoding=f(p)\\
PE &\in \mathcal{R}^d \\
p &\in [1, 2, 3, \dots]
\end{cases}
$$

它将位置实数映射成$d$维向量，其中$d$通常写成$d_{model}$，表示它与transformer模型的输入维度一致。注意到编码算式中的$k=10000$是一个非常大的数，此时的编码图样为如下左图，横坐标为模型维度$d_{model}$，纵坐标为位置标号$p$。同时注意到位置编码在前几个维度上变化较大，而在后面的维度上变化较小，这是一个固定的现象。

![Untitled](Transformer%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%20335cb853f67245dbb326c0ee6d546d5a/Untitled.png)

当我令$k=10$时，位置编码的“影响范围”就变得更大。直观想来，这会是影响模型后续分析的可变参数，$k$值越大，则位置编码所占的“比重”就越小。当然，这只是我的猜想。

![Untitled](Transformer%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%20335cb853f67245dbb326c0ee6d546d5a/Untitled%201.png)

上图的右侧子图是不同位置$p$编码之间的“距离矩阵”，度量高维向量之间距离的方式有很多，这里采用模范数

$$
\vert PE_{p1} - PE_{p2} \vert
$$

为了让比较更加直观，我还计算了它们之间的相关系数作为横坐标，

$$
corr(PE_{p1}, PE_{p2})
$$

形成了如下点云。点云中的每个点代表距离矩阵的元素，左图代表上三角部分，右图代表下三角部分。点云图表示此种位置编码在两种距离度量下，不同位置之间的差异是“相似”的。

![Untitled](Transformer%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%20335cb853f67245dbb326c0ee6d546d5a/Untitled%202.png)

最后将几个典型位置的编码与其他位置的码距离绘制出来，以供后查。

![Untitled](Transformer%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%20335cb853f67245dbb326c0ee6d546d5a/Untitled%203.png)

本文的代码可见我的ObservableHQ笔记本。

[Positional Encoding in Transformer](https://observablehq.com/@listenzcc/positional-encoding-in-transformer)
