# GPT-4o在知识类问题上更强

Title: GPT-4o在知识类问题上更强
Date: 2024-5-15
Keywords: LLM

---

以GPT为代表的LLM，能够从已经生成的海量内容中提炼人类思维的知识，这是LLM成为新时代“操作系统”的底层能力之一。本文的例子表明，新推出的GPT-4o作为知识引擎的能力更强，表现在回答知识类问题时的能力更强，它既知道这些知识，也知道如何利用它们。

---

[toc]

## 机器学习和人的思维有何不同

人与机器最大的区别就是人会变通、会主动地因地制宜，而机器很呆，只会根据事先写定的条条框框进行计算。

比如，在二战战场上，有人想通过缴获敌军坦克的序号估计敌军投入的坦克数量。根据最大后验概率（MAP）准则，坦克数量就是最大的序号。但这个结果显然不对，因为**在采样数量较小的情况下，“恰好”缴获敌军坦克最大序号的概率并不大，甚至接近这个序号的概率也不大**。为了解决这个矛盾，有聪明人提出另一种无偏的估计方法，并且在战场上得到了验证。因此，这类问题也称为“德国坦克”问题。

在本文中，我把这个问题分别提问了GPT-3.5和GPT-4o，在明确要求它用“德国坦克”问题思路求解的情况下，前者没有给出合格的答案（见附录），而新的GPT-4o则给出了正确的答案（见问题求解部分）。由此可见，GPT-4o在知识类问题上更强。

另外，就事论事地比较“德国坦克问题”的估计方法和最大后验概率（MAP）方法，并探讨它们的应用，可以揭示机器学习方法与人类思维在解决问题时的一些关键差异。

- **德国坦克问题的方法**：这种方法基于特定的数学推理和公式来调整样本的极值，以估计总体的上限。它是一种专门设计的统计推断方法，体现了基于数学推理的人的思维方式。它通过特定公式进行简单的校正，适合特定类型的问题（如样本量小的情况下估计总体最大值）。这种方法的逻辑和步骤非常明确，但应用范围有限。
- **最大后验概率（MAP）方法**：MAP方法依赖于贝叶斯统计，通过结合先验知识和观测数据进行估计。这种方法展示了人类在解决问题时利用背景知识和数据的综合能力。MAP方法可以适应不同的先验知识，这使得它更具灵活性和适应性。对于没有明确先验的情况，可以使用非信息先验，使得方法更广泛适用。

由此可以引申出机器学习和人类思维的些许不同：

- **人类思维**：更倾向于模型驱动的方法，依赖逻辑推理、经验和背景知识来解决问题。人类思维能够在信息有限的情况下，通过逻辑推理和直觉进行决策。人类也擅长在解决新问题时迅速调整和应用已有知识。通常具有更好的解释性和透明性。人类能够解释为什么做出某个决定，基于逻辑和经验提供推理过程。例如，德国坦克问题的方法和MAP方法都是基于明确的数学和逻辑推理过程。具有很强的创造性和泛化能力。人类能够在不同背景和少量信息的情况下，提出新的解决方案和创新方法。
- **机器学习**：主要依赖数据驱动的方法，通过大量的数据进行训练，自动提取特征并进行预测。机器学习方法的优势在于能够处理海量数据，自动发现模式，而不依赖于具体的数学公式或推理过程。例如，神经网络可以通过不断调整参数来优化性能，尽管其中的具体逻辑可能无法完全被人类解释。机器学习有时被视为“黑箱”方法，因为某些复杂模型（如深度学习）的内部工作原理难以解释。尽管这些模型的预测性能可能很高，但缺乏透明性和解释性。
- **LLM知识引擎**：以GPT为代表的LLM，能够从已经生成的海量内容中提炼人类思维的知识，这是LLM成为新时代“操作系统”的底层能力之一。本文的例子表明，新推出的GPT-4o作为知识引擎的能力更强，表现在回答知识类问题时的能力更强，它既知道这些知识，也知道如何利用它们。

## 问题求解

考虑一个简单的问题

> 对随机变量有一组观测样本，分别是$1,7,10,13,18$，若它满足平均分布，则分布的上、下限分别是多少？

GPT-4o首先给出了MAP的结果。我不满意，于是又问

> 用“德国坦克”问题的思路重新求解该问题

以下内容整理自它的回答。至于“德国坦克”问题的证明，可见我之前写的内容。

[平均分布的范围估计原理](https://mp.weixin.qq.com/s?__biz=MzkxNTI1MDc5NA==&mid=2247487277&idx=1&sn=5ade62c5a7cbcd765c08388c1b6857ba&chksm=c1634428f614cd3eeb1c6b6a8b989576211aabfe191ec68679d349fb2de87302b3f8b238e543#rd)

### 机器的惯常推断：最大后验概率（MAP）

对于均匀分布，所有样本在区间 $[a, b]$上均匀取值。均匀分布 $𝑈(𝑎,𝑏)$ 的概率密度函数是：

$$
p(x) = \begin{cases}
\frac{1}{b-a}, & \text{if } a \leq x \leq b \\
0, & \text{otherwise}
\end{cases}
$$

给定一组观测样本并假设这些样本满足均匀分布，我们需要确定均匀分布的上、下限，即 $𝑎$ 和  $𝑏$，使得这些样本都在这个范围内均匀分布。用最大后验概率的思路求解该问题的方法为最大化似然函数

$$
\mathcal{L}(a, b \vert x_1, x_2, \dots x_n) = \frac{1}{(b-a)^n}
$$

由于上限 $b$应该至少大于等于样本中的最大值，下限 $a$应该至多小于等于样本中的最小值。因此，在确定样本的最小值（$=1$）和最大值（$=18$）的情况下，均匀分布的区间是 $[1, 18]$。

### 更像人脑的特化推断：扩展估计

"德国坦克"问题是一种统计推断方法，用于估计总体中最大值的实际上限。在二战期间，盟军用该方法估计德国坦克的生产数量。我们可以用类似的思路来估计均匀分布的上限和下限。假设我们有一组观测样本 $x_1, x_2, \ldots, x_n$ 来自一个均匀分布。根据德国坦克问题的思路，我们可以估计最大值 $b$  和最小值 $a$ 。具体来说，对于均匀分布 $U(a, b)$：使用样本最大值$M$来估计$b$ 。根据德国坦克问题的公式，$b$的无偏估计量如下式，

$$
\hat{b} = M + \frac{M - m}{n - 1}
$$

其中$M$是样本中的最大值，$m$是样本中的最小值，$n$ 是样本的数量。接下来使用样本最小值 $m$ 来估计 $a$。均匀分布的最小值$a$ 可以直接用样本最小值来估计，因为它是已知的：

$$
\hat{a} = m
$$

现在应用到我们的样本数据上，$1, 7, 10, 13, 18$：

- 样本最小值，$m = 1$
- 样本最大值，$M = 18$
- 样本数量，$n = 5$

首先估计$b$：

$$
\hat{b} = M + \frac{M - m}{n - 1} = 18 + \frac{18 - 1}{5 - 1} = 18 + \frac{17}{4} = 18 + 4.25 = 22.25
$$

然后，最小值$a$直接用样本最小值估计：

$$
\hat{a} = 1
$$

因此，基于德国坦克问题的思路，均匀分布的区间估计为$[1, 22.25]$。

### 总结与方法比较

- **最大后验概率（MAP）方法**：在假设非信息先验的情况下，MAP方法的结果与样本的最大值和最小值直接相等，这意味着它没有进行进一步的修正。适用于样本量足够大且对先验知识没有特定要求的情况。
- **德国坦克问题的方法**：假设观测样本是从均匀分布中独立随机抽取的。使用样本的最大值和最小值来推断均匀分布的上、下限。该方法的估计量是无偏的，通过调整样本最大值来推测更大范围的总体最大值，更适用于样本量较小的情况，因为它考虑到了样本的有限性带来的偏差。

## 附录：GPT-3.5 Turbo的回答

### **Sider:** GPT-3.5 Turbo

![Untitled](GPT-4o%E5%9C%A8%E7%9F%A5%E8%AF%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8A%E6%9B%B4%E5%BC%BA%20a76b50a6c01a4422bbf05327abb1b77c/Untitled.png)

![Untitled](GPT-4o%E5%9C%A8%E7%9F%A5%E8%AF%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8A%E6%9B%B4%E5%BC%BA%20a76b50a6c01a4422bbf05327abb1b77c/Untitled%201.png)

### Monica

![Untitled](GPT-4o%E5%9C%A8%E7%9F%A5%E8%AF%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8A%E6%9B%B4%E5%BC%BA%20a76b50a6c01a4422bbf05327abb1b77c/Untitled%202.png)

![Untitled](GPT-4o%E5%9C%A8%E7%9F%A5%E8%AF%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8A%E6%9B%B4%E5%BC%BA%20a76b50a6c01a4422bbf05327abb1b77c/Untitled%203.png)
