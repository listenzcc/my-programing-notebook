---
Title: 读图易，分析难：大模型的计量单位 petaFLOP
Date: 2024-9-19
Keywords: LLM, ViT, petaFLOP
---

本文介绍了计算性能单位 PetaFLOP (PF) 的应用及其在深度学习中的意义。PetaFLOP 表示一个计算机在每秒钟能执行`10的15次方`次浮点运算。在 GPU 硬件方面，当前的 A100 GPU 集群需要 8 张卡才能提供 5 PF-days 的计算能力，而下一代 700 系列 GPU 将能提供更高的计算能力。

对于 1080p 图像，遍历一次全部像素所需的计算量特别小，数量级是`10的负10次方`。但以 Vision Transformer (ViT) 为例，该模型处理图像时使用固定大小的子块将244尺寸的图像分割成约200个token。以GPT3为例，它生成 100 个 token 的计算需求大约为 0.035 petaFLOPs，也就是说理解一张图，或者说用100个字描述一张图需要约 0.07 petaFLOPs的计算量，这显示出处理大规模模型的高计算需求。

Transformer的最大贡献是让 0.07 petaFLOPs的快速计算成为可能。

---

[toc]

## 单位PF-days

如果你读了OpenAI发布在`arXiv`上的几篇论文，一定会注意到他们对模型优化的所需算力的单位，它称为PF-days。
这是非常有意思的单位。
这个东西和电力计费的逻辑差不多，以指某个设备以`1 petaFLOPs`每秒的速度计算一天消耗的计算量。

> A petaFLOP/s-day (or pf day) is a unit of measurement that represents the total number of floating-point operations that can be performed by a computer with a processing speed of one petaFLOP (10^15 operations per second) over the duration of one day. In other words, it's the total compute operations if you run at that speed for 24 hours.

其中，peta是一种大数的量纲，$1peta=10^{12}$。

## 对应的GPU硬件

以目前的A100 GPU卡为例，需要将`8`张卡绑一块才能提供`5 PF-days`的计算能力。下一代的700系列GPU卡会以140张卡的集群提供700 petaFLOPs的计算能力，换算起来是`1`张卡就能提供`5 PF-days`的计算能力。

> **DGX A100 Technical Specifications**
>
> Eight NVIDIA A100 Tensor Core GPUs, delivering 5 petaFLOPs of AI power, with 320GB in total GPU memory with 12.4TB per second in bandwidth.
>
> **NVIDIA Builds Next-Gen 700 petaFLOPs DGX SuperPOD**
>
> NVIDIA also revealed its [next-generation DGX SuperPOD](https://blogs.nvidia.com/blog/2020/05/14/dgx-superpod-a100), a cluster of 140 DGX A100 systems capable of achieving 700 petaFLOPs of AI computing power.

[NVIDIA Ships World’s Most Advanced AI System — NVIDIA DGX A100 — to Fight COVID-19; Third-Generation DGX Packs Record 5 petaFLOPs of AI Performance](https://nvidianews.nvidia.com/news/nvidia-ships-worlds-most-advanced-ai-system-nvidia-dgx-a100-to-fight-covid-19-third-generation-dgx-packs-record-5-petaFLOPs-of-ai-performance)

## 1080p图像的消耗

> For a 1080p image, which has a resolution of 1920x1080 pixels, the total number of pixels is 2,073,600. If you are performing basic operations (like addition, subtraction, etc.) on each pixel's color channel (typically 3 channels for RGB), the total number of floating-point computations would depend on the specific operation performed per pixel. For example, if you perform one computation per channel per pixel, you'd have:
>
> `2,073,600 pixels x 3 channels = 6,220,800 floating-point` computations for a single operation across all pixels and channels.

要将 6,220,800 floating-point 操作（FLOP）转换成 PetaFLOP，需要利用如下换算式：

$$
1 \text{PetaFLOP} = 10^{15} \text{FLOP}
$$

所以，

$$
6,220,800 \text{ FLOP} = \frac{6,220,800}{10^{15}} \text{ PF} = 6.2208 \times 10^{-10} \text{ PF}
$$

因此，遍历1080p图像的全部像素一次的操作，换算成 PetaFLOP（PF）是 $6.2208 \times 10^{-10}$ PF。

## ViT的消耗

Vision Transformer (ViT)是利用LLM进行图像理解的模型，结果先行，处理一张图像所需的PF量是`700`，远远大于遍历图像的全部像素所需的计算量。该模型的做法是将图像**划分成固定大小的子块（patches）**，然后对这些子块进行处理。具体步骤如下：

1. **图像分块**：ViT 会将输入图像划分为大小固定的子块（通常是$16 \times 16$ 像素块）。例如，对于一个 $224 \times 224$ 像素的图像，图像会被划分成$\frac{224}{16} \times \frac{224}{16} = 14 \times 14 = 196$ 个子块。
2. **线性投影**：每个子块被展平成一个向量。对于 $16 \times 16$ 的子块，这个展平后的向量长度为 $16 \times 16 \times 3 = 768$（假设是 RGB 图像，每个像素 `3` 个通道）。然后通过一个线性投影将每个展平的子块映射到固定维度的向量空间中（通常也是 `768` 维），类似于在 NLP 中将词映射到词向量。
3. **位置编码**：由于 Transformer 没有内建的空间信息（与 CNN 不同），为了捕捉图像块之间的相对位置，ViT 会为每个子块添加一个位置编码。这帮助模型了解子块的空间顺序和相互关系。
4. **Transformer 编码器**：然后，这些包含位置编码的子块作为输入被送入标准的 Transformer 编码器中，Transformer 使用自注意力机制来捕捉各个子块之间的全局依赖关系。自注意力机制允许模型关注图像中不同部分的关系，而不是局限于局部特征。
5. **分类标识符（CLS token）**：在输入子块的序列前面，ViT 通常会插入一个额外的分类标识符（CLS token），用于最终的分类任务。通过 Transformer 的编码过程后，CLS token 会积累整个图像的全局信息，最后用于分类头（MLP）进行图像分类。
6. **分类**：最后通过一个 MLP 层对 Transformer 编码后的 CLS token 进行处理，输出图像的分类结果。

总结来说，ViT 是通过将图像划分成固定大小的子块（patches），并将这些子块作为“词”（类似 NLP 的 token）输入 Transformer，利用自注意力机制来处理图像中的全局信息。对于ViT中的一张图像，**token 的数量**等于图像被划分成的子块数量加上一个额外的分类标识符（CLS token）。因此，对于一张`224 x 224`像素的图像，子块大小为`16 x 16` 的 ViT 模型会有 `197`个 tokens。

## 生成100个token的消耗

计算每生成100个token所需的FLOPS涉及几个核心变量，特别是模型的规模和架构。以GPT-3为例，以下是一个近似估算：

1. **模型参数**：GPT-3有1750亿个参数。
2. **每个token的推理过程**：每生成一个token时，整个模型都参与推理，通常需要对每一层做大量的矩阵乘法和其他操作。计算FLOPS时，矩阵乘法是最主要的开销。

一个近似的公式为：

$$
\text{FLOPS} = 2 \times (\text{模型参数数量}) \times (\text{序列长度}) \times (\text{生成的token数})
$$

其中：

- `2` 是矩阵乘法中涉及的浮点运算数量因子。
- 序列长度是当前输入和生成的token数量之和，因为每个生成的token要基于之前的所有token进行计算。

对于GPT-3，假设生成的token序列长度约为100，并且生成100个新token，那么生成每个token的浮点运算大约为：

$$
\text{FLOPS} = 2 \times 1750亿 \times 100 \times 100 = 3.5 \times 10^{12} \text{ FLOPS}
$$

因此，生成`100`个token大约需要  $35 \times 10^{12}$  FLOPS，也就是`35`TeraFLOPS。这个估计非常粗略，实际的FLOPS消耗可能会略有不同，取决于硬件优化、模型精确度和具体任务的复杂性。
